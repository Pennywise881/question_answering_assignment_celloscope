{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Pennywise881/question_answering_assignment_celloscope.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -nc -q {\"https://nlp.stanford.edu/data/glove.6B.zip\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pwd\n",
    "# %cd /content/question_answering_assignment_celloscope\n",
    "# %mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip '/home/nafi/Documents/Work/question_answering_assignment_celloscope/glove.6B.zip' -d '/home/nafi/Documents/Work/question_answering_assignment_celloscope/glove_6B' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements_new.txt\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 05:21:44.930174: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-04 05:21:44.930194: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# external libraries\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "# from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# internal utilities\n",
    "import config\n",
    "from model import BiDAF\n",
    "from data_loader import SquadDataset\n",
    "from utils import save_checkpoint, compute_batch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(config.train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing values used for training\n",
    "prepro_params = {\n",
    "    \"max_words\": config.max_words,\n",
    "    \"word_embedding_size\": config.word_embedding_size,\n",
    "    \"char_embedding_size\": config.char_embedding_size,\n",
    "    \"max_len_context\": config.max_len_context,\n",
    "    \"max_len_question\": config.max_len_question,\n",
    "    \"max_len_word\": config.max_len_word\n",
    "}\n",
    "\n",
    "# hyper-parameters setup\n",
    "hyper_params = {\n",
    "    \"num_epochs\": config.num_epochs,\n",
    "    \"batch_size\": config.batch_size,\n",
    "    \"learning_rate\": config.learning_rate,\n",
    "    \"hidden_size\": config.hidden_size,\n",
    "    \"char_channel_width\": config.char_channel_width,\n",
    "    \"char_channel_size\": config.char_channel_size,\n",
    "    \"drop_prob\": config.drop_prob,\n",
    "    \"cuda\": config.cuda,\n",
    "    \"pretrained\": config.pretrained\n",
    "}\n",
    "\n",
    "experiment_params = {\"preprocessing\": prepro_params, \"model\": hyper_params}\n",
    "\n",
    "# train on GPU if CUDA variable is set to True (a GPU with CUDA is needed to do so)\n",
    "device = torch.device(\"cuda\" if hyper_params[\"cuda\"] else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# define a path to save experiment logs\n",
    "experiment_path = \"output/{}\".format(config.exp)\n",
    "if not os.path.exists(experiment_path):\n",
    "    os.mkdir(experiment_path)\n",
    "\n",
    "# save the preprocesisng and model parameters used for this training experiemnt\n",
    "with open(os.path.join(experiment_path, \"config_{}.json\".format(config.exp)), \"w\") as f:\n",
    "    json.dump(experiment_params, f)\n",
    "\n",
    "# start TensorboardX writer\n",
    "# writer = SummaryWriter(experiment_path)\n",
    "\n",
    "# open features file and store them in individual variables (train + dev)\n",
    "train_features = np.load(os.path.join(config.train_dir, \"train_features.npz\"), allow_pickle=True)\n",
    "t_w_context, t_c_context, t_w_question, t_c_question, t_labels = train_features[\"context_idxs\"],\\\n",
    "                                                                 train_features[\"context_char_idxs\"],\\\n",
    "                                                                 train_features[\"question_idxs\"],\\\n",
    "                                                                 train_features[\"question_char_idxs\"],\\\n",
    "                                                                 train_features[\"label\"]\n",
    "\n",
    "dev_features = np.load(os.path.join(config.dev_dir, \"dev_features.npz\"), allow_pickle=True)\n",
    "d_w_context, d_c_context, d_w_question, d_c_question, d_labels = dev_features[\"context_idxs\"],\\\n",
    "                                                                 dev_features[\"context_char_idxs\"],\\\n",
    "                                                                 dev_features[\"question_idxs\"],\\\n",
    "                                                                 dev_features[\"question_char_idxs\"],\\\n",
    "                                                                 dev_features[\"label\"]\n",
    "\n",
    "# load the embedding matrix created for our word vocabulary\n",
    "with open(os.path.join(config.train_dir, \"word_embeddings.pkl\"), \"rb\") as e:\n",
    "    word_embedding_matrix = pickle.load(e)\n",
    "with open(os.path.join(config.train_dir, \"char_embeddings.pkl\"), \"rb\") as e:\n",
    "    char_embedding_matrix = pickle.load(e)\n",
    "\n",
    "# load mapping between words and idxs\n",
    "with open(os.path.join(config.train_dir, \"word2idx.pkl\"), \"rb\") as f:\n",
    "    word2idx = pickle.load(f)\n",
    "\n",
    "idx2word = dict([(y, x) for x, y in word2idx.items()])\n",
    "\n",
    "# transform them into Tensors\n",
    "word_embedding_matrix = torch.from_numpy(np.array(word_embedding_matrix)).type(torch.float32)\n",
    "char_embedding_matrix = torch.from_numpy(np.array(char_embedding_matrix)).type(torch.float32)\n",
    "\n",
    "# load datasets\n",
    "# train_dataset = SquadDataset(t_w_context, t_c_context, t_w_question, t_c_question, t_labels)\n",
    "# valid_dataset = SquadDataset(d_w_context, d_c_context, d_w_question, d_c_question, d_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to change the data as some labels are empty and this creates an error in the DataLoader. Therefore I have to find and select only those rows from the dataset that have 2 labels i.e. a start position and an end position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encodings(word_context, char_context, word_question, char_question, labels):\n",
    "    encodings = {'word_context':None, 'char_context':None, 'word_question':None, 'char_question':None, 'labels':None}\n",
    "    empty_label_indices = []\n",
    "    for i in range(len(labels)):\n",
    "        if not len(list(labels[i])):\n",
    "            empty_label_indices.append(i)\n",
    "\n",
    "    encodings['word_context'] = np.delete(word_context, empty_label_indices, 0)\n",
    "    encodings['char_context'] = np.delete(char_context, empty_label_indices, 0)\n",
    "    encodings['word_question'] = np.delete(word_question, empty_label_indices, 0)\n",
    "    encodings['char_question'] = np.delete(char_question, empty_label_indices, 0)\n",
    "    encodings['labels'] = np.delete(labels, empty_label_indices, 0)\n",
    "\n",
    "    return encodings\n",
    "\n",
    "train_encodings = get_encodings(t_w_context, t_c_context, t_w_question, t_c_question, t_labels)\n",
    "val_encodings = get_encodings(d_w_context, d_c_context, d_w_question, d_c_question, d_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(t_w_context[0]))\n",
    "# print(type(train_encodings['word_context'][0]))\n",
    "# # print(t_w_context[0])\n",
    "# # print(train_encodings['word_context'][0])\n",
    "# # print(type(train_encodings['char_context']))\n",
    "# # print(type(train_encodings['word_question']))\n",
    "# # print(type(train_encodings['char_question']))\n",
    "# # print(type(train_encodings['labels']))\n",
    "# # print(type(t_c_context[0]))\n",
    "# print(train_encodings['word_context'].shape)\n",
    "# print(train_encodings['char_context'].shape)\n",
    "# print(train_encodings['word_question'].shape)\n",
    "# print(train_encodings['char_question'].shape)\n",
    "# print(train_encodings['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_encodings['word_context']))\n",
    "# print(len(train_encodings['questions']))\n",
    "# print(len(train_encodings['labels']))\n",
    "\n",
    "# class SquadDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings):\n",
    "#         self.encodings = encodings\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.encodings['word_context'])\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "valid_dataset = SquadDataset(val_encodings)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# for i, batch in enumerate(train_loader):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.utils.data as data\n",
    "\n",
    "\n",
    "# class SquadDataset(data.Dataset):\n",
    "#     \"\"\"Custom Dataset for SQuAD data compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "\n",
    "#     def __init__(self, w_context, c_context, w_question, c_question, labels):\n",
    "#         \"\"\"Set the path for context, question and labels.\"\"\"\n",
    "#         self.w_context = w_context\n",
    "#         self.c_context = c_context\n",
    "#         self.w_question = w_question\n",
    "#         self.c_question = c_question\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         \"\"\"Returns one data tuple of the form ( word context, character context, word question,\n",
    "#          character question, answer).\"\"\"\n",
    "#         # return self.w_context[index], self.c_context[index], self.w_question[index], self.c_question[index],\\\n",
    "#         #        self.labels[index]\n",
    "#         return self.w_context[index], self.w_question[index], self.labels[index]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.w_context)\n",
    "\n",
    "# train_dataset = SquadDataset(t_w_context, t_c_context, t_w_question, t_c_question, t_labels)\n",
    "# valid_dataset = SquadDataset(d_w_context, d_c_context, d_w_question, d_c_question, d_labels)\n",
    "\n",
    "# print(train_encodings.keys())\n",
    "# print(len(train_encodings['context']))\n",
    "# print(len(train_encodings['questions'][0]))\n",
    "# print(len(train_encodings['labels']))\n",
    "# print(\"Word context shape:\", t_w_context[0].shape)\n",
    "# print(\"Char context shape:\", t_c_context.shape)\n",
    "# print(\"Word question shape:\", t_w_question.shape)\n",
    "# print(\"Char question shape:\", t_c_question.shape)\n",
    "# print(\"Labels shape:\", t_labels.shape)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# foo = next(iter(train_loader))['context']\n",
    "# print(foo.shape)\n",
    "# for i, batch in enumerate(train_loader):\n",
    "#     print(batch['context'].shape)\n",
    "#     print(batch['question'].shape)\n",
    "    # print(batch['question'].shape)\n",
    "    # print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data loader is: 1353\n",
      "Length of valid data loader is: 92\n"
     ]
    }
   ],
   "source": [
    "# # load data generators\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle=True,\n",
    "                              batch_size=hyper_params[\"batch_size\"],\n",
    "                              num_workers=4)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset,\n",
    "                              shuffle=True,\n",
    "                              batch_size=hyper_params[\"batch_size\"],\n",
    "                              num_workers=4)\n",
    "\n",
    "print(\"Length of training data loader is:\", len(train_dataloader))\n",
    "print(\"Length of valid data loader is:\", len(valid_dataloader))\n",
    "\n",
    "# load the model\n",
    "model = BiDAF(word_vectors=word_embedding_matrix,\n",
    "              char_vectors=char_embedding_matrix,\n",
    "              hidden_size=hyper_params[\"hidden_size\"],\n",
    "              drop_prob=hyper_params[\"drop_prob\"])\n",
    "if hyper_params[\"pretrained\"]:\n",
    "    model.load_state_dict(torch.load(os.path.join(experiment_path, \"model.pkl\"))[\"state_dict\"])\n",
    "model.to(device)\n",
    "\n",
    "# define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), hyper_params[\"learning_rate\"], weight_decay=1e-4)\n",
    "\n",
    "# best loss so far\n",
    "if hyper_params[\"pretrained\"]:\n",
    "    best_valid_loss = torch.load(os.path.join(experiment_path, \"model.pkl\"))[\"best_valid_loss\"]\n",
    "    epoch_checkpoint = torch.load(os.path.join(experiment_path, \"model_last_checkpoint.pkl\"))[\"epoch\"]\n",
    "    print(\"Best validation loss obtained after {} epochs is: {}\".format(epoch_checkpoint, best_valid_loss))\n",
    "else:\n",
    "    best_valid_loss = 100\n",
    "    epoch_checkpoint = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "tensor(9.8299, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# train_dataloader = DataLoader(train_dataset,\n",
    "#                               # shuffle=True,\n",
    "#                               batch_size=2)\n",
    "#                               # num_workers=4)\n",
    "\n",
    "# print(\"Length of training data loader is:\", len(train_dataloader))\n",
    "\n",
    "# print(train_dataset.__getitem__(5)[0].shape)\n",
    "# print(train_dataset.__getitem__(5)[1].shape)\n",
    "# print(train_dataset.__getitem__(5)[2].shape)\n",
    "# print(train_dataset.__getitem__(5)[3].shape)\n",
    "# print(train_dataset.__getitem__(5)[4].shape)\n",
    "# print(train_dataset.__getitem__(5)[4])\n",
    "\n",
    "# # print(train_dataloader.__len__())\n",
    "with torch.no_grad():\n",
    "  # loop = tqdm(train_dataloader, position=0)\n",
    "  print(device)\n",
    "  for i, batch in enumerate(train_dataloader):\n",
    "    # pass\n",
    "    w_context, c_context, w_question, c_question, label1, label2 = batch['word_context'].long().to(device),\\\n",
    "                                                                    batch['char_context'].long().to(device), \\\n",
    "                                                                    batch['word_question'].long().to(device), \\\n",
    "                                                                    batch['char_question'].long().to(device), \\\n",
    "                                                                    batch['labels'][:, 0].long().to(device),\\\n",
    "                                                                    batch['labels'][:, 1].long().to(device)\n",
    "    pred1, pred2 = model(w_context, c_context, w_question, c_question)\n",
    "    loss = criterion(pred1, label1) + criterion(pred2, label2)\n",
    "    print(loss)\n",
    "    # w_context\n",
    "    # print(w_context.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('worknlpenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2500674b868a17b4aed2f4e171c0d947f29e6abbd0acab0d541b110a8940df07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
